{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Web Scraping to Gain Company Insights\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data.\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content.\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, \"html.parser\")\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified |  My wife and I are very disappo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified |  We flew BA between Heathrow an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not Verified |  Absolutely disgusted with BA. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified | Took a trip to Nashville with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Verified |  A nightmare journey courtesy o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  Not Verified |  My wife and I are very disappo...\n",
       "1  Not Verified |  We flew BA between Heathrow an...\n",
       "2  Not Verified |  Absolutely disgusted with BA. ...\n",
       "3  Not Verified | Took a trip to Nashville with m...\n",
       "4  Not Verified |  A nightmare journey courtesy o..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(os.getcwd(), \"data\")):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "df.to_csv(\"data/BA_reviews_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Sentiments with Rule-Based Approach\n",
    "\n",
    "This is the traditional way to do sentiment analysis based on a set of manually-created rules. This approach includes NLP techniques like lexicons (lists of words), stemming, tokenization and parsing.\n",
    "\n",
    "Widely used lexicon-based approaches are TextBlob, VADER, SentiWordNet. We will be using VADER approach for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Cleaning\n",
    "\n",
    "We will clean the data to remove the \"<b>✅ Trip Verified</b>\" text, then convert the text into lower-case, and remove all the punctuation marks from the data. This is essential as punctuation marks do not add any meaningful value for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife and I are very disappointed with fly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We flew BA between Heathrow and Berlin one w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely disgusted with BA. Our flights we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Took a trip to Nashville with my wife for a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A nightmare journey courtesy of British Airw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0    My wife and I are very disappointed with fly...\n",
       "1    We flew BA between Heathrow and Berlin one w...\n",
       "2    Absolutely disgusted with BA. Our flights we...\n",
       "3   Took a trip to Nashville with my wife for a l...\n",
       "4    A nightmare journey courtesy of British Airw..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"reviews\"] = df[\"reviews\"].str.split(\"|\", expand=True)[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife and I are very disappointed with fly...</td>\n",
       "      <td>my wife and i are very disappointed with fly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We flew BA between Heathrow and Berlin one w...</td>\n",
       "      <td>we flew ba between heathrow and berlin one w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absolutely disgusted with BA. Our flights we...</td>\n",
       "      <td>absolutely disgusted with ba our flights wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Took a trip to Nashville with my wife for a l...</td>\n",
       "      <td>took a trip to nashville with my wife for a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A nightmare journey courtesy of British Airw...</td>\n",
       "      <td>a nightmare journey courtesy of british airw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    My wife and I are very disappointed with fly...   \n",
       "1    We flew BA between Heathrow and Berlin one w...   \n",
       "2    Absolutely disgusted with BA. Our flights we...   \n",
       "3   Took a trip to Nashville with my wife for a l...   \n",
       "4    A nightmare journey courtesy of British Airw...   \n",
       "\n",
       "                                     cleaned_reviews  \n",
       "0    my wife and i are very disappointed with fly...  \n",
       "1    we flew ba between heathrow and berlin one w...  \n",
       "2    absolutely disgusted with ba our flights wer...  \n",
       "3   took a trip to nashville with my wife for a l...  \n",
       "4    a nightmare journey courtesy of british airw...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def case_and_punctuation_handling(text):\n",
    "    lower_case_text = text.lower()\n",
    "    cleaned_text = lower_case_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return cleaned_text\n",
    "\n",
    "df[\"cleaned_reviews\"] = df[\"reviews\"].apply(case_and_punctuation_handling)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization, Stopwords removal, and POS-tagging\n",
    "\n",
    "1) Tokenization breaks text(or sentences, in our case) into smaller parts(or individual words) for easier machine analysis, helping machines understand human language. Tokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens. So here, we will tokenize our text data to convert it into smaller tokens(or words).\n",
    "\n",
    "2) Stopwords are a set of commonly used words in a language. Examples of stop words in English are \"a\", \"the\", \"is\", \"are\", etc. Stopwords are commonly used in Natural Language Processing (NLP) to eliminate words that are so widely used that carry very little useful information. We will remove such stopwords from our data going forward.\n",
    "\n",
    "3) Part-of-speech (POS) tagging is fundamental in natural language processing (NLP) and it involves labelling words in a sentence with their corresponding POS tags having form (word, tag). POS tags indicate the grammatical category of a word, such as noun, verb, adjective, adverb, etc. The goal of POS tagging is to determine a sentence’s syntactic structure and identify each word’s role in the sentence. POS tagging is essential to preserve the context of the word and is essential for Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloading all nltk packages to avoid compatibility issues.\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging dictionary.\n",
    "pos_dict = {\"J\":wordnet.ADJ, \"V\":wordnet.VERB, \"N\":wordnet.NOUN, \"R\":wordnet.ADV}\n",
    "\n",
    "def handle_tokenization_stopwords_postags(text):\n",
    "    # Tokenization.\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    # POS-tagging.\n",
    "    tags = pos_tag(tokenized_text)\n",
    "    new_text_list = []\n",
    "    for word, tag in tags:\n",
    "        # Stopword removal.\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "          new_text_list.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return new_text_list \n",
    "\n",
    "df[\"POS_tagged_reviews\"] = df[\"cleaned_reviews\"].apply(handle_tokenization_stopwords_postags)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item, generally known as stem. A stem is responsible for the word's lexical meaning. The two popular techniques of obtaining the stem words are Stemming and Lemmatization. \n",
    "   \n",
    "We will use the Lemmatization process here as Stemming often gives some meaningless root words as it simply chops off some characters in the end, whereas, Lemmatization gives meaningful root words. It is important to note that, lemmatization requires POS tagging of the words, which we have already accomplished in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining stem-words with Lemmatization.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_lemmatization(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "     if not pos:\n",
    "        lemma = word\n",
    "        lemma_rew = lemma_rew + \" \" + lemma\n",
    "     else:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "df[\"lemmatized_reviews\"] = df[\"POS_tagged_reviews\"].apply(text_lemmatization)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Sentiment Analysis (VADER)\n",
    "  \n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analyzer that has been trained on social media text. Here, SentimentIntensityAnalyzer() is an object and polarity_scores is a method which will  give us scores of the following categories:\n",
    "\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "- Compound\n",
    "   \n",
    "The compound score is the sum of positive, negative & neutral scores which is then normalized between -1(most extreme negative) and +1 (most extreme positive). The more Compound score closer to +1, the higher the positivity of the text. For our Analyser, we will consider compound score of >=0.5 as positive, and compund score of <0 as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_analysis(sentiment_text):\n",
    "    score = analyzer.polarity_scores(sentiment_text)\n",
    "    return score[\"compound\"]\n",
    "\n",
    "df[\"VADER_compound_score\"] = df[\"lemmatized_reviews\"].apply(vader_analysis)\n",
    "\n",
    "# function to analyse\n",
    "def sentiment_classifier(compound_score):\n",
    "    if compound_score >= 0.5:\n",
    "        return \"Positive\"\n",
    "    elif compound_score < 0 :\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df[\"Sentiment\"] = df[\"VADER_compound_score\"].apply(sentiment_classifier)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = df[\"Sentiment\"].value_counts()\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/BA_reviews_analysed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vizualization: Pie-Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "  \n",
    "# Declaring data. \n",
    "data = sentiment_counts.values\n",
    "  \n",
    "# Define Seaborn color palette to use. \n",
    "palette_color = sns.color_palette() \n",
    "  \n",
    "# Plotting data on chart. \n",
    "plt.pie(data, labels=sentiment_counts.index, colors=palette_color, explode = (0, 0, 0.15), autopct=\"%.1f%%\") \n",
    "  \n",
    "# Displaying chart. \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vizualization: Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def vizualize_wordcloud(data):\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=200, max_font_size=40, scale=4, relative_scaling=0.9, random_state=7)\n",
    "    wordcloud = wordcloud.generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "vizualize_wordcloud(df[\"lemmatized_reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## References:\n",
    "\n",
    "1) https://analyticsindiamag.com/sentiment-analysis-made-easy-using-vader/   \n",
    "2) https://www.geeksforgeeks.org/python-lemmatization-with-nltk/   \n",
    "3) https://www.kaggle.com/code/mchirico/quick-look-seaborn-wordcloud   \n",
    "4) https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/#h-using-vader   \n",
    "5) https://pypi.org/project/beautifulsoup4/   \n",
    "6) https://www.airlinequality.com/airline-reviews/british-airways   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
